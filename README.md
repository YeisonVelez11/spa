# John Deere Parts Scraper

Aplicaci√≥n para extraer informaci√≥n de partes y modelos del cat√°logo de John Deere.

## Instalaci√≥n

```bash
npm install
```

## Configuraci√≥n

1. Copia el archivo `.env.example` a `.env`:
```bash
cp .env.example .env
```

2. Completa las variables de entorno en `.env`:

```env
# Captcha Token de John Deere (obtenerlo del navegador)
CAPTCHA_TOKEN=tu_captcha_token_aqui

# Cookies de sesi√≥n (obtenerlas del navegador)
COOKIES=tu_cookie_string_completo_aqui

# Google Drive API Credentials (opcional)
# Si SERVER est√° definido, los CSV e im√°genes se guardar√°n en Google Drive
SERVER=true
GOOGLE_CLIENT_EMAIL=tu_service_account_email@project.iam.gserviceaccount.com
GOOGLE_PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\ntu_private_key_aqui\n-----END PRIVATE KEY-----\n"
```

## Modos de Ejecuci√≥n

### 1. Modo CLI (L√≠nea de Comandos)

Ejecuta el proceso directamente desde la terminal:

```bash
node jhondeere.js
```

O con un archivo espec√≠fico:

```bash
node jhondeere.js 1
```

Este modo:
- ‚úÖ Procesa las piezas del archivo especificado
- ‚úÖ Muestra el progreso en consola
- ‚úÖ Guarda archivos seg√∫n la configuraci√≥n de `SERVER`

### 2. Modo Servidor (API REST)

Inicia el servidor Express:

```bash
node server.js
```

El servidor estar√° disponible en `http://localhost:3000`

#### Endpoints Disponibles:

**Health Check:**
```bash
GET /health
```
Retorna el estado del servidor y la hora actual.

Respuesta:
```json
{
  "status": "OK",
  "timestamp": "28/10/2025, 22:30:00",
  "uptime": 123.456,
  "message": "Server is running"
}
```

**Iniciar Proceso:**
```bash
GET /process/start
```
Inicia el proceso de scraping en segundo plano.

Respuesta:
```json
{
  "status": "started",
  "message": "El proceso ha iniciado. Revisa la consola para ver el progreso.",
  "timestamp": "28/10/2025, 22:30:00"
}
```

**Ver Datos:**
```bash
GET /ver/:nombreArchivo
```
Visualiza los datos de un archivo JSON en formato tabla.

**Descargar Archivos:**
```bash
GET /descargar/:nombreArchivo/csv
GET /descargar/:nombreArchivo/json
```

## Comportamiento seg√∫n Configuraci√≥n

### Sin `SERVER` (Modo Local)
- üìÅ CSV se guardan en `./csv_output/`
- üìÅ Im√°genes se guardan en `./images/`
- ‚è±Ô∏è Operaciones bloqueantes (espera a que termine cada guardado)

### Con `SERVER=true` (Modo Google Drive)
- ‚òÅÔ∏è CSV se suben a Google Drive en carpetas espec√≠ficas:
  - `modelos`: Carpeta con ID especificado en constante `modelos`
  - `piezas`: Carpeta con ID especificado en constante `piezas`
  - `partes`: Carpeta con ID especificado en constante `partes`
- ‚òÅÔ∏è Im√°genes se suben a carpeta con ID especificado en constante `images`
- ‚ö° Operaciones no bloqueantes (contin√∫a sin esperar las subidas)
- üîÑ Autenticaci√≥n √∫nica y global (solo se autentica una vez)

## Estructura de Archivos

```
spa/
‚îú‚îÄ‚îÄ jhondeere.js          # Script principal de scraping
‚îú‚îÄ‚îÄ server.js             # Servidor Express con API
‚îú‚îÄ‚îÄ funciones.js          # Funciones auxiliares
‚îú‚îÄ‚îÄ package.json          # Dependencias
‚îú‚îÄ‚îÄ .env                  # Variables de entorno (no incluido en repo)
‚îú‚îÄ‚îÄ .env.example          # Plantilla de variables de entorno
‚îú‚îÄ‚îÄ data/                 # Archivos de entrada con IDs de piezas
‚îú‚îÄ‚îÄ csv_output/           # CSV generados (modo local)
‚îú‚îÄ‚îÄ images/               # Im√°genes descargadas (modo local)
‚îî‚îÄ‚îÄ front-lib/            # Librer√≠as frontend para visualizaci√≥n
```

## Notas Importantes

- El proceso incluye reintentos infinitos para manejar errores de red y API
- Los headers HTTP se rotan autom√°ticamente para evitar bloqueos
- El delay entre peticiones es adaptativo seg√∫n los errores recibidos
- La autenticaci√≥n de Google Drive se realiza una sola vez y se reutiliza
- En modo servidor, el proceso se ejecuta en segundo plano sin bloquear el endpoint

## Desarrollo

Para desarrollo con auto-reload:

```bash
nodemon server.js
```

o

```bash
nodemon jhondeere.js
```
